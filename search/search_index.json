{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Asgard GPU Cluster","text":"<p>Asgard is a small-scale GPU cluster prototype designed for reproducible, containerized machine learning and high-performance workloads.</p> <p>The system combines:</p> <ul> <li>Slurm for job scheduling</li> <li>Enroot and Pyxis for containerized execution</li> <li>A structured file system for data management</li> <li>Grafana for monitoring GPU usage and system health</li> </ul> <p>This documentation explains both how to use the cluster and how it is built.</p>"},{"location":"#asgard-facts","title":"Asgard facts","text":"<ul> <li>GPUs: 4 GPUs (NVIDIA RTX Pro 6000 Server)</li> <li>CPU: AMD Epic Turin CPU (96 CPU Cores)</li> <li>Memory: 1.5 TiB RAM</li> <li>System Storage: 480GB \u00d7 2 SSD</li> <li>Data Storage: 3TB \u00d7 4 HDD NVMe RAID0</li> <li>Workload Manager: Slurm Workload Manager</li> <li>Containers: Enroot container environments</li> <li>Monitoring: Grafana monitoring suite</li> </ul>"},{"location":"#connecting-to-vpn","title":"Connecting to VPN","text":"<p>To access the Asgard GPU cluster, you must first connect to the university VPN.</p> <p>The cluster is hosted on the internal university network and is not accessible from the public internet. A VPN connection is required before attempting to:</p> <ul> <li>SSH into the cluster</li> <li>Submit Slurm jobs</li> <li>Access cluster services such as Grafana</li> </ul> <p>Ensure that the VPN connection is active and verified before connecting to the cluster.</p>"},{"location":"admin-guide/","title":"Admin Guide","text":""},{"location":"admin-guide/#user-management","title":"User Management","text":"<ul> <li>Users are created using standard Linux tools</li> <li>Slurm controls resource access</li> </ul>"},{"location":"admin-guide/#slurm-services","title":"Slurm Services","text":"<ul> <li>slurmctld (controller)</li> <li>slurmd (compute daemon)</li> <li>munge (authentication)</li> </ul>"},{"location":"admin-guide/#container-management","title":"Container Management","text":"<ul> <li>Base Enroot images are centrally managed</li> <li>Users may create personal containers</li> </ul>"},{"location":"architecture/","title":"System Architecture","text":""},{"location":"architecture/#what-is-a-compute-node","title":"What is a Compute Node?","text":"<p>A compute node is a physical machine that provides CPUs, GPUs, memory, and storage for running jobs. In the current prototype, Asgard consists of a single node equipped with NVIDIA GPUs.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ul> <li>Slurm: Schedules jobs and allocates GPUs and CPUs</li> <li>Enroot: Provides lightweight, rootless containers</li> <li>Pyxis: Integrates Enroot directly into Slurm jobs</li> <li>File System: Separates permanent data from temporary job data</li> <li>Grafana: Visualizes system and GPU metrics</li> </ul>"},{"location":"architecture/#execution-flow","title":"Execution Flow","text":"<ol> <li>User submits a Slurm job</li> <li>Slurm allocates GPUs and CPUs</li> <li>Pyxis launches the job inside an Enroot container</li> <li>The job reads/writes data using the cluster file system</li> <li>Metrics are collected and visualized in Grafana</li> </ol>"},{"location":"containers/","title":"Containers: Enroot and Pyxis","text":"<p>Asgard uses Enroot containers to provide isolated, reproducible software environments for GPU workloads. Containers ensure that users can install their own dependencies without affecting the system or other users.</p> <p>Enroot is integrated with Slurm through Pyxis, allowing containers to be launched automatically when jobs are scheduled.</p>"},{"location":"containers/#why-containers-are-used","title":"Why Containers Are Used","text":"<p>Containers in Asgard are used to:</p> <ul> <li>Avoid dependency conflicts between users</li> <li>Ensure reproducibility of experiments</li> <li>Provide GPU access without root privileges</li> <li>Keep the host operating system stable</li> </ul> <p>Users should assume that all compute jobs run inside containers.</p>"},{"location":"containers/#container-model-in-asgard","title":"Container Model in Asgard","text":"<ul> <li>Containers run as the user, not as root</li> <li>Containers share the host kernel</li> <li>GPUs are passed through automatically</li> <li>File system paths such as <code>$HOME</code> and <code>/netscratch</code> are visible inside containers</li> </ul> <p>Installed software inside a container persists across runs.</p>"},{"location":"containers/#importing-a-container-image","title":"Importing a Container Image","text":"<p>Asgard supports importing container images directly from Docker registries such as NVIDIA NGC.</p>"},{"location":"containers/#example-import-a-pytorch-image","title":"Example: Import a PyTorch Image","text":"<pre><code>enroot import docker://nvcr.io/nvidia/pytorch:23.10-py3\n</code></pre>"},{"location":"containers/#software-stack","title":"Software Stack","text":"<p>When submitting jobs with Slurm and Pyxis, container behavior is controlled using container-specific options. The --container-image option defines the container environment in which the job runs, ensuring reproducible software dependencies. The --container-mounts option exposes selected directories from the host system inside the container, allowing access to user files, scratch space, and shared datasets. Finally, --container-workdir specifies the directory inside the container where execution begins, aligning the container\u2019s context with the job\u2019s working directory.</p> <pre><code>--container-image=[USER@][REGISTRY#]IMAGE[:TAG]|PATH\n                        [pyxis] the image to use for the container\n                        filesystem. Can be either a docker image given as\n                        an enroot URI, or a path to a squashfs file on the\n                        remote host filesystem.\n\n--container-mounts=SRC:DST[:FLAGS][,SRC:DST...]\n                        [pyxis] bind mount[s] inside the container. Mount\n                        flags are separated with \"+\", e.g. \"ro+rprivate\"\n\n--container-workdir=PATH\n</code></pre>"},{"location":"filesystem/","title":"File System Layout","text":"<p>Asgard uses a structured file system to separate permanent data from temporary job-related data.</p>"},{"location":"filesystem/#home","title":"$HOME","text":"<ul> <li>Purpose: Source code, configuration files, final results</li> <li>Backed up</li> <li>Persistent across sessions</li> </ul>"},{"location":"filesystem/#netscratch","title":"/netscratch","text":"<ul> <li>Purpose: Active job data, intermediate results, checkpoints</li> <li>Shared across the cluster</li> <li>Not backed up</li> </ul>"},{"location":"filesystem/#fscratch","title":"/fscratch","text":"<ul> <li>Purpose: Low-latency temporary storage for highly distributed jobs</li> <li>Local to each compute node</li> <li>Not backed up</li> </ul>"},{"location":"filesystem/#ds","title":"/ds","text":"<ul> <li>Purpose: Shared datasets</li> <li>Read-only</li> <li>Users must not write to this location</li> </ul>"},{"location":"monitoring/","title":"Monitoring with Grafana","text":"<p>Asgard uses Grafana to monitor the health and performance of the GPU workstation/cluster. Monitoring provides visibility into resource usage and helps ensure that jobs are running efficiently and correctly.</p> <p>At the current stage, Grafana is used as the primary monitoring and visualization tool.</p>"},{"location":"monitoring/#why-monitoring-is-important","title":"Why Monitoring Is Important","text":"<p>Monitoring allows administrators and users to:</p> <ul> <li>Observe GPU utilization during Slurm jobs</li> <li>Detect idle or underutilized GPUs</li> <li>Monitor system load and resource usage</li> <li>Identify potential performance bottlenecks</li> <li>Validate that allocated resources are being used as expected</li> </ul> <p>Without monitoring, GPU resources can be silently wasted or misconfigured jobs can go unnoticed.</p>"},{"location":"monitoring/#monitoring-architecture-overview","title":"Monitoring Architecture Overview","text":"<p>The monitoring setup in Asgard consists of:</p> <ul> <li>Grafana running as a centralized visualization service</li> <li>Metrics collected from the system and GPU stack</li> <li>Dashboards displaying real-time and historical usage</li> </ul> <p>Grafana provides a single interface to observe system behavior during job execution.</p>"},{"location":"monitoring/#grafana","title":"Grafana","text":"<p>Grafana is a web-based monitoring and visualization platform.</p> <p>In Asgard, Grafana is used to:</p> <ul> <li>Visualize GPU utilization and memory usage</li> <li>Monitor CPU and system load</li> <li>Observe resource usage trends over time</li> <li>Support debugging of Slurm jobs and workloads</li> </ul> <p>Grafana dashboards present cluster metrics in an intuitive and accessible way.</p>"},{"location":"monitoring/#relationship-to-slurm","title":"Relationship to Slurm","text":"<p>Monitoring complements Slurm scheduling:</p> <ul> <li>Slurm controls resource allocation</li> <li>Grafana shows resource utilization</li> </ul> <p>For example: - Slurm allocates a GPU to a job - Grafana shows whether that GPU is actively used or idle</p> <p>This helps identify inefficient jobs or configuration issues.</p>"},{"location":"monitoring/#access-policy","title":"Access Policy","text":"<ul> <li>Grafana dashboards are read-only for users</li> <li>Dashboard configuration is managed by administrators</li> <li>Users are encouraged to consult Grafana to understand job behavior</li> </ul> <p>Monitoring does not allow users to interfere with other jobs or system operation.</p>"},{"location":"monitoring/#current-scope-and-future-extensions","title":"Current Scope and Future Extensions","text":"<p>At the current stage of Asgard:</p> <ul> <li>Grafana is deployed and operational</li> <li>Monitoring focuses on system- and GPU-level metrics</li> <li>The setup is suitable for a single-node prototype</li> </ul> <p>In future phases, this setup can be extended with: - More detailed GPU metrics - Job-level attribution - Alerts and notifications - Integration with cluster-wide monitoring components</p>"},{"location":"monitoring/#summary","title":"Summary","text":"<p>Grafana provides essential visibility into the Asgard GPU environment, enabling both administrators and users to understand how resources are used during Slurm jobs. Even in its initial form, monitoring is a key component of a stable and efficient GPU cluster.</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#implemented","title":"Implemented","text":"<ul> <li>Slurm scheduling</li> <li>Enroot and Pyxis integration</li> <li>Structured file system</li> <li>Grafana monitoring</li> </ul>"},{"location":"roadmap/#planned","title":"Planned","text":"<ul> <li>Multi-node expansion</li> <li>Parallel file system (BeeGFS)</li> <li>Job accounting and fair-share scheduling</li> <li>Improved documentation and examples</li> </ul>"},{"location":"slurm/","title":"Slurm Job Scheduling","text":"<p>Slurm is the workload manager used by Asgard to schedule and execute jobs on the GPU cluster. Users do not run heavy workloads directly on the login node; instead, they submit jobs to Slurm, which allocates GPUs, CPUs, and memory in a controlled and reproducible manner.</p>"},{"location":"slurm/#core-concepts","title":"Core Concepts","text":""},{"location":"slurm/#jobs","title":"Jobs","text":"<p>A job is a unit of work submitted to Slurm. Each job requests specific resources such as GPUs, CPUs, memory, and runtime.</p>"},{"location":"slurm/#nodes-and-gpus","title":"Nodes and GPUs","text":"<ul> <li>A node is a physical machine.</li> <li>A GPU is an accelerator attached to a node.</li> <li>Slurm allocates GPUs using GRES (Generic Resources).</li> </ul>"},{"location":"slurm/#job-id","title":"Job ID","text":"<p>Each submitted job is assigned a unique Job ID, available inside the job as the environment variable <code>$SLURM_JOB_ID</code>.</p>"},{"location":"slurm/#submitting-a-job","title":"Submitting a Job","text":"<p>Jobs are submitted using the <code>sbatch</code> command with a job script.</p>"},{"location":"slurm/#example-job-script","title":"Example Job Script","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=asgard-train\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=01:00:00\n#SBATCH --output=logs/%x-%j.out\n\n# Create a job-specific working directory\nJOB_DIR=/netscratch/$USER/job_$SLURM_JOB_ID\nmkdir -p \"$JOB_DIR\"\ncd \"$JOB_DIR\"\n\n# Run training inside the allocated resources\npython train.py\n\n</code></pre>"},{"location":"slurm/#add-account-parameter-and-srunsbatch","title":"ADD ACCOUNT PARAMETER AND SRUN/SBATCH","text":""},{"location":"user-guide/","title":"User Workflow","text":""},{"location":"user-guide/#typical-workflow","title":"Typical Workflow","text":"<ol> <li>Log in to the cluster</li> <li>Prepare code in <code>$HOME</code></li> <li>Use or create an Enroot container</li> <li>Submit a Slurm job</li> <li>Monitor job progress</li> <li>Store final results in <code>$HOME</code></li> </ol> <p>This workflow ensures reproducibility and efficient resource usage.</p>"}]}