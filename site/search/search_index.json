{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Asgard GPU Cluster","text":"<p>Asgard is a small-scale GPU cluster prototype designed for reproducible, containerized machine learning and high-performance workloads.</p> <p>The system combines: - Slurm for job scheduling - Enroot and Pyxis for containerized execution - A structured file system for data management - Grafana for monitoring GPU usage and system health</p> <p>This documentation explains both how to use the cluster and how it is built.</p>"},{"location":"admin-guide/","title":"Admin Guide","text":""},{"location":"admin-guide/#user-management","title":"User Management","text":"<ul> <li>Users are created using standard Linux tools</li> <li>Slurm controls resource access</li> </ul>"},{"location":"admin-guide/#slurm-services","title":"Slurm Services","text":"<ul> <li>slurmctld (controller)</li> <li>slurmd (compute daemon)</li> <li>munge (authentication)</li> </ul>"},{"location":"admin-guide/#container-management","title":"Container Management","text":"<ul> <li>Base Enroot images are centrally managed</li> <li>Users may create personal containers</li> </ul>"},{"location":"architecture/","title":"System Architecture","text":""},{"location":"architecture/#what-is-a-compute-node","title":"What is a Compute Node?","text":"<p>A compute node is a physical machine that provides CPUs, GPUs, memory, and storage for running jobs. In the current prototype, Asgard consists of a single node equipped with NVIDIA GPUs.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ul> <li>Slurm: Schedules jobs and allocates GPUs and CPUs</li> <li>Enroot: Provides lightweight, rootless containers</li> <li>Pyxis: Integrates Enroot directly into Slurm jobs</li> <li>File System: Separates permanent data from temporary job data</li> <li>Grafana: Visualizes system and GPU metrics</li> </ul>"},{"location":"architecture/#execution-flow","title":"Execution Flow","text":"<ol> <li>User submits a Slurm job</li> <li>Slurm allocates GPUs and CPUs</li> <li>Pyxis launches the job inside an Enroot container</li> <li>The job reads/writes data using the cluster file system</li> <li>Metrics are collected and visualized in Grafana</li> </ol>"},{"location":"containers/","title":"Containers: Enroot and Pyxis","text":"<p>Asgard uses Enroot containers to provide isolated, reproducible software environments for GPU workloads. Containers ensure that users can install their own dependencies without affecting the system or other users.</p> <p>Enroot is integrated with Slurm through Pyxis, allowing containers to be launched automatically when jobs are scheduled.</p>"},{"location":"containers/#why-containers-are-used","title":"Why Containers Are Used","text":"<p>Containers in Asgard are used to:</p> <ul> <li>Avoid dependency conflicts between users</li> <li>Ensure reproducibility of experiments</li> <li>Provide GPU access without root privileges</li> <li>Keep the host operating system stable</li> </ul> <p>Users should assume that all compute jobs run inside containers.</p>"},{"location":"containers/#container-model-in-asgard","title":"Container Model in Asgard","text":"<ul> <li>Containers run as the user, not as root</li> <li>Containers share the host kernel</li> <li>GPUs are passed through automatically</li> <li>File system paths such as <code>$HOME</code> and <code>/netscratch</code> are visible inside containers</li> </ul> <p>Installed software inside a container persists across runs.</p>"},{"location":"containers/#importing-a-container-image","title":"Importing a Container Image","text":"<p>Asgard supports importing container images directly from Docker registries such as NVIDIA NGC.</p>"},{"location":"containers/#example-import-a-pytorch-image","title":"Example: Import a PyTorch Image","text":"<pre><code>enroot import docker://nvcr.io/nvidia/pytorch:23.10-py3\n</code></pre>"},{"location":"filesystem/","title":"File System Layout","text":"<p>Asgard uses a structured file system to separate permanent data from temporary job-related data.</p>"},{"location":"filesystem/#home","title":"$HOME","text":"<ul> <li>Purpose: Source code, configuration files, final results</li> <li>Backed up</li> <li>Persistent across sessions</li> </ul>"},{"location":"filesystem/#netscratch","title":"/netscratch","text":"<ul> <li>Purpose: Active job data, intermediate results, checkpoints</li> <li>Shared across the cluster</li> <li>Not backed up</li> </ul>"},{"location":"filesystem/#fscratch","title":"/fscratch","text":"<ul> <li>Purpose: Low-latency temporary storage for highly distributed jobs</li> <li>Local to each compute node</li> <li>Not backed up</li> </ul>"},{"location":"filesystem/#ds","title":"/ds","text":"<ul> <li>Purpose: Shared datasets</li> <li>Read-only</li> <li>Users must not write to this location</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#implemented","title":"Implemented","text":"<ul> <li>Slurm scheduling</li> <li>Enroot and Pyxis integration</li> <li>Structured file system</li> <li>Grafana monitoring</li> </ul>"},{"location":"roadmap/#planned","title":"Planned","text":"<ul> <li>Multi-node expansion</li> <li>Parallel file system (BeeGFS)</li> <li>Job accounting and fair-share scheduling</li> <li>Improved documentation and examples</li> </ul>"},{"location":"slurm/","title":"Slurm Job Scheduling","text":"<p>Slurm is the workload manager used by Asgard to schedule and execute jobs on the GPU cluster. Users do not run heavy workloads directly on the login node; instead, they submit jobs to Slurm, which allocates GPUs, CPUs, and memory in a controlled and reproducible manner.</p>"},{"location":"slurm/#core-concepts","title":"Core Concepts","text":""},{"location":"slurm/#jobs","title":"Jobs","text":"<p>A job is a unit of work submitted to Slurm. Each job requests specific resources such as GPUs, CPUs, memory, and runtime.</p>"},{"location":"slurm/#nodes-and-gpus","title":"Nodes and GPUs","text":"<ul> <li>A node is a physical machine.</li> <li>A GPU is an accelerator attached to a node.</li> <li>Slurm allocates GPUs using GRES (Generic Resources).</li> </ul>"},{"location":"slurm/#job-id","title":"Job ID","text":"<p>Each submitted job is assigned a unique Job ID, available inside the job as the environment variable <code>$SLURM_JOB_ID</code>.</p>"},{"location":"slurm/#submitting-a-job","title":"Submitting a Job","text":"<p>Jobs are submitted using the <code>sbatch</code> command with a job script.</p>"},{"location":"slurm/#example-job-script","title":"Example Job Script","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=asgard-train\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=01:00:00\n#SBATCH --output=logs/%x-%j.out\n\n# Create a job-specific working directory\nJOB_DIR=/netscratch/$USER/job_$SLURM_JOB_ID\nmkdir -p \"$JOB_DIR\"\ncd \"$JOB_DIR\"\n\n# Run training inside the allocated resources\npython train.py\n\n</code></pre>"},{"location":"user-guide/","title":"User Workflow","text":""},{"location":"user-guide/#typical-workflow","title":"Typical Workflow","text":"<ol> <li>Log in to the cluster</li> <li>Prepare code in <code>$HOME</code></li> <li>Use or create an Enroot container</li> <li>Submit a Slurm job</li> <li>Monitor job progress</li> <li>Store final results in <code>$HOME</code></li> </ol> <p>This workflow ensures reproducibility and efficient resource usage.</p>"}]}